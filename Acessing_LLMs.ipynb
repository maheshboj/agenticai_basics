{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshboj/agenticai_basics/blob/Langchain_components/Acessing_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-openai\n",
        "%pip install langchain-community\n",
        "%pip install openai\n",
        "%pip install -U langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwCzA99WlYpt",
        "outputId": "ad803532-1728-4156-aafc-344fa316a1ad"
      },
      "id": "gwCzA99WlYpt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.2 (from langchain-openai)\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.2->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.2->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.80\n",
            "    Uninstalling langchain-core-0.3.80:\n",
            "      Successfully uninstalled langchain-core-0.3.80\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.1.0 langchain-openai-1.0.3\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.1.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.1.0 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.15.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Downloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.0.8 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-google-genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P3m4zJwKJyF8",
        "outputId": "71a986f3-1852-41cc-8820-abd5351f3c92"
      },
      "id": "P3m4zJwKJyF8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<1.0.0,>=0.9.0 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.10)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.5->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-3.1.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain-google-genai-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "641c6b27953a4787a6877e2f8a3a23e8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep lang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eroyy4L6xhFI",
        "outputId": "2bf54564-e0b4-4285-f3a4-3b300b6cb54c"
      },
      "id": "eroyy4L6xhFI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google-ai-generativelanguage             0.9.0\n",
            "google-cloud-language                    2.18.0\n",
            "langchain                                1.0.8\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4.1\n",
            "langchain-core                           1.1.0\n",
            "langchain-google                         0.1.1\n",
            "langchain-google-genai                   3.1.0\n",
            "langchain-openai                         1.0.3\n",
            "langchain-text-splitters                 1.0.0\n",
            "langgraph                                1.0.3\n",
            "langgraph-checkpoint                     3.0.1\n",
            "langgraph-prebuilt                       1.0.5\n",
            "langgraph-sdk                            0.2.9\n",
            "langsmith                                0.4.43\n",
            "libclang                                 18.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the API Keys Safely\n",
        "\n",
        "Securing API keys is a critical aspect of software development. Exposing your keys can lead to unauthorized access to your accounts, resulting in data breaches, unexpected charges, and service disruption.\n",
        "\n",
        "The single most important rule is: **Never hardcode API keys directly in your source code.**\n",
        "\n",
        "Here are the best methods to use API keys securely, from most to least recommended:\n",
        "\n",
        "### 1. Secrets Management Services (Most Secure - Best for Production)\n",
        "\n",
        "For production applications, especially in a team environment, using a dedicated secrets management service is the gold standard. These services are designed to handle sensitive data securely.\n",
        "\n",
        "*   **How they work:** They store your secrets (like API keys) in an encrypted vault. Your application is given permission (often through a role or service account) to request the secret at runtime. The key is never stored in your code or on the server's file system.\n",
        "*   **Examples:**\n",
        "    *   AWS Secrets Manager\n",
        "    *   Google Secret Manager\n",
        "    *   HashiCorp Vault\n",
        "    *   Azure Key Vault"
      ],
      "metadata": {
        "id": "hyveuiMpkfXz"
      },
      "id": "hyveuiMpkfXz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2f4399",
      "metadata": {
        "id": "5e2f4399"
      },
      "outputs": [],
      "source": [
        "# from getpass import getpass\n",
        "# import os\n",
        "\n",
        "# GOOGLE_API_KEY = getpass('Enter Open AI API Key: ')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4339ed8b",
      "metadata": {
        "id": "4339ed8b"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.tools import WikipediaQueryRun\n",
        "# from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=8000)\n",
        "# wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api_wrapper, features=\"lxml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "ADvEQ48hbs63"
      },
      "id": "ADvEQ48hbs63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_KEY2')"
      ],
      "metadata": {
        "id": "rtdRtm1zbFGg"
      },
      "id": "rtdRtm1zbFGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Native Libraries for Accesing Models"
      ],
      "metadata": {
        "id": "AkODDLlRlJIU"
      },
      "id": "AkODDLlRlJIU"
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Create OpenAI client using existing OPENAI_API_KEY from environment\n",
        "client_openai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "openai_response = client_openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain how AI works in a few words\"}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(openai_response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TQ-sPKIlIlr",
        "outputId": "dd8ad854-0e7f-4038-867b-97de51e6c735"
      },
      "id": "5TQ-sPKIlIlr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI works by using algorithms and data to simulate human-like intelligence, enabling machines to learn, reason, and make decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in client_openai.models.list():\n",
        "  print(i.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOfiCGQ7nMLK",
        "outputId": "920d1e30-d170-446c-d406-c2f4d3ba6305"
      },
      "id": "BOfiCGQ7nMLK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4-0613\n",
            "gpt-4\n",
            "gpt-3.5-turbo\n",
            "gpt-5.1-codex-mini\n",
            "gpt-5.1-chat-latest\n",
            "gpt-5.1-2025-11-13\n",
            "gpt-5.1\n",
            "gpt-5.1-codex\n",
            "davinci-002\n",
            "babbage-002\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "dall-e-3\n",
            "dall-e-2\n",
            "gpt-4-1106-preview\n",
            "gpt-3.5-turbo-1106\n",
            "tts-1-hd\n",
            "tts-1-1106\n",
            "tts-1-hd-1106\n",
            "text-embedding-3-small\n",
            "text-embedding-3-large\n",
            "gpt-4-0125-preview\n",
            "gpt-4-turbo-preview\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-4-turbo\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4o\n",
            "gpt-4o-2024-05-13\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-mini\n",
            "gpt-4o-2024-08-06\n",
            "chatgpt-4o-latest\n",
            "gpt-4o-realtime-preview-2024-10-01\n",
            "gpt-4o-audio-preview-2024-10-01\n",
            "gpt-4o-audio-preview\n",
            "gpt-4o-realtime-preview\n",
            "omni-moderation-latest\n",
            "omni-moderation-2024-09-26\n",
            "gpt-4o-realtime-preview-2024-12-17\n",
            "gpt-4o-audio-preview-2024-12-17\n",
            "gpt-4o-mini-realtime-preview-2024-12-17\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "o1-2024-12-17\n",
            "o1\n",
            "gpt-4o-mini-realtime-preview\n",
            "gpt-4o-mini-audio-preview\n",
            "o3-mini\n",
            "o3-mini-2025-01-31\n",
            "gpt-4o-2024-11-20\n",
            "gpt-4o-search-preview-2025-03-11\n",
            "gpt-4o-search-preview\n",
            "gpt-4o-mini-search-preview-2025-03-11\n",
            "gpt-4o-mini-search-preview\n",
            "gpt-4o-transcribe\n",
            "gpt-4o-mini-transcribe\n",
            "o1-pro-2025-03-19\n",
            "o1-pro\n",
            "gpt-4o-mini-tts\n",
            "o3-2025-04-16\n",
            "o4-mini-2025-04-16\n",
            "o3\n",
            "o4-mini\n",
            "gpt-4.1-2025-04-14\n",
            "gpt-4.1\n",
            "gpt-4.1-mini-2025-04-14\n",
            "gpt-4.1-mini\n",
            "gpt-4.1-nano-2025-04-14\n",
            "gpt-4.1-nano\n",
            "gpt-image-1\n",
            "gpt-4o-realtime-preview-2025-06-03\n",
            "gpt-4o-audio-preview-2025-06-03\n",
            "gpt-4o-transcribe-diarize\n",
            "gpt-5-chat-latest\n",
            "gpt-5-2025-08-07\n",
            "gpt-5\n",
            "gpt-5-mini-2025-08-07\n",
            "gpt-5-mini\n",
            "gpt-5-nano-2025-08-07\n",
            "gpt-5-nano\n",
            "gpt-audio-2025-08-28\n",
            "gpt-realtime\n",
            "gpt-realtime-2025-08-28\n",
            "gpt-audio\n",
            "gpt-5-codex\n",
            "gpt-image-1-mini\n",
            "gpt-5-pro-2025-10-06\n",
            "gpt-5-pro\n",
            "gpt-audio-mini\n",
            "gpt-audio-mini-2025-10-06\n",
            "gpt-5-search-api\n",
            "gpt-realtime-mini\n",
            "gpt-realtime-mini-2025-10-06\n",
            "sora-2\n",
            "sora-2-pro\n",
            "gpt-5-search-api-2025-10-14\n",
            "gpt-3.5-turbo-16k\n",
            "tts-1\n",
            "whisper-1\n",
            "text-embedding-ada-002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfba98d",
      "metadata": {
        "id": "dbfba98d"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ce2b7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ce2b7c",
        "outputId": "a4b72cfe-a23a-4e46-d0cf-2eaa38663edc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='1. **Data-Driven Learning**: AI systems use large datasets to learn patterns and make decisions. Through techniques like machine learning, they analyze data to improve their performance on specific tasks without being explicitly programmed for each scenario.\\n\\n2. **Automation and Efficiency**: AI automates complex processes, enhancing efficiency and accuracy in various fields such as healthcare, finance, and transportation. It can perform repetitive tasks, analyze vast amounts of data quickly, and provide insights that would be time-consuming for humans.\\n\\n3. **Adaptive and Evolving**: AI technologies are designed to adapt and evolve over time. They can update their algorithms based on new data, allowing them to improve their accuracy and effectiveness continuously. This adaptability makes AI a powerful tool for solving dynamic and complex problems.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 16, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_7eeb46f068', 'id': 'chatcmpl-CedcEa8alMX0vEqUPBVOhAWI7NmM6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--106c5001-dc17-4cc4-836d-3ace49ec3bd9-0', usage_metadata={'input_tokens': 16, 'output_tokens': 153, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "llm.invoke(\"Explain the AI in 3 Bullet Points.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['AZURE_OPENAI_ENDPOINT']=\"https://techm-training-4050-resource.openai.azure.com/openai/v1/\"\n",
        "os.environ['AZURE_OPENAI_API_KEY']=userdata.get('AZURE_OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "6Z-XJCK2dJmZ"
      },
      "id": "6Z-XJCK2dJmZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",  # Your Azure deployment name\n",
        "    base_url=\"https://techm-training-4050-resource.openai.azure.com/openai/v1/\",\n",
        "    api_key=userdata.get('AZURE_OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Hello, how are you?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wzviawtmi1p",
        "outputId": "e120e1bf-560a-4a38-b825-a3dcfe8f13c4"
      },
      "id": "4Wzviawtmi1p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help with whatever you need! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "Opk_oLFZpPyb"
      },
      "id": "Opk_oLFZpPyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model_llm=init_chat_model(model='gpt-4o',base_url=\"https://techm-training-4050-resource.openai.azure.com/openai/v1/\",\n",
        "                    api_key=userdata.get('AZURE_OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "2EPqCk4rpPvB"
      },
      "id": "2EPqCk4rpPvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model_llm.invoke('what is AI in 1 sentence')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlu6TZOHpPr7",
        "outputId": "c7c43759-0222-494e-b13a-9455634ed53f"
      },
      "id": "zlu6TZOHpPr7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='AI, or artificial intelligence, is the simulation of human intelligence in machines designed to perform tasks like learning, reasoning, problem-solving, and decision-making.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 14, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CedcMEhsNMpWPhp5nag1rpBqMn5Eo', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--186a0a1e-9d5a-4e50-bb5f-5090a361b84e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 31, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n",
        "]\n",
        "\n",
        "response = llm.invoke(conversation)\n",
        "print(response)  # AIMessage(\"J'adore créer des applications.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0QEg9HFpPo-",
        "outputId": "2f06f24d-45ee-42cd-ca1f-f10fff676e6d"
      },
      "id": "G0QEg9HFpPo-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"J'adore créer des applications.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 48, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CedcNE5Gydk1EAYRMWpMhHqDhM1Ov', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--340be7d3-b599-426f-bc3a-1d6105dcdb82-0' usage_metadata={'input_tokens': 48, 'output_tokens': 7, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(\"Why do parrots have colorful feathers?\"):\n",
        "    print(chunk.text, end=\"|\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZU_AYcHpPlv",
        "outputId": "71e1c5bc-0bb1-43bb-d4fe-0496e6b39b62"
      },
      "id": "jZU_AYcHpPlv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||Par|rots| have| colorful| feathers| for| several| biological| and| evolutionary| reasons|:\n",
            "\n",
            "|1|.| **|At|tract|ing| M|ates|**|:| The| vibrant| colors| often| play| a| crucial| role| in| court|ship| and| mate| selection|.| Bright|ly| colored| feathers| signal| good| health| and| strong| genetic| fitness|,| making| the| par|rot| more| desirable| to| potential| mates|.\n",
            "\n",
            "|2|.| **|Cam|ouflage|**|:| Although| it| may| seem| counter|int|uitive|,| the| colorful| plum|age| can| actually| help| parro|ts| blend| into| their| natural| environments|,| such| as| the| bright| greens|,| reds|,| and| yell|ows| of| rain|fore|sts|.| Their| feathers| mimic| the| patterns| and| colors| found| in| the| foliage| and| flowers| where| they| live|,| making| them| less| visible| to| predators|.\n",
            "\n",
            "|3|.| **|Communication|**|:| Par|rots| use| their| colorful| feathers| as| a| form| of| communication| with| other| members| of| their| species|.| Certain| colors| or| feather| displays| may| signal| aggression|,| readiness| to| mate|,| or| territorial| behavior|.\n",
            "\n",
            "|4|.| **|Ther|mal| Regulation|**|:| Bright| colors| can| play| a| role| in| controlling| heat| absorption|.| Depending| on| the| habitat|,| feather| colors| might| help| with| reflecting| or| absorbing| heat|.\n",
            "\n",
            "|5|.| **|Species| Identification|**|:| The| unique| patterns| and| colors| help| parro|ts| identify| members| of| their| own| species|,| which| is| important| for| social| interactions| and| reproduction|.| \n",
            "\n",
            "|Overall|,| their| colorful| plum|age| is| an| adaptive| trait| shaped| by| evolution| to| ensure| survival| and| reproduction| in| their| specific| environment|.||"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses = llm.batch([\n",
        "    \"Why do parrots have colorful feathers?\",\n",
        "    \"How do airplanes fly?\",\n",
        "    \"What is quantum computing?\"\n",
        "])\n",
        "for response in responses:\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XVHrWYxpPij",
        "outputId": "265abce5-81a6-4669-e041-5d8edbac5238"
      },
      "id": "6XVHrWYxpPij",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Parrots have colorful feathers primarily for purposes related to survival and reproduction. Here are the main reasons for their vibrant colors:\\n\\n### 1. **Mating and Social Signals**\\n   - Bright and colorful feathers help parrots attract mates. These vibrant colors can signal health, vitality, and genetic fitness to potential partners. A bird with brighter feathers is often perceived as more capable of producing strong offspring.\\n   - Coloration can also play a role in identifying individual birds, species, or genders, which helps within social groups and during courtship displays.\\n\\n### 2. **Camouflage**\\n   - While it may seem counterintuitive, bright feathers can sometimes act as camouflage in their natural habitats, like tropical rainforests. Many parrots live in areas with dense, colorful foliage, flowers, and light-dappled environments, where their vivid plumage blends in rather than stands out.\\n   \\n### 3. **Communication**\\n   - Parrots use their feathers as a visual tool for communication. Bright plumage can convey mood or serve as a warning signal to other animals or members of their species.\\n\\n### 4. **Structural Feather Composition**\\n   - The coloration in parrot feathers is often due to the structure of their feathers rather than pigmentation alone. Special microscopic structures within the feather refract and interact with light to create bright, iridescent colors.\\n\\n### 5. **Evolutionary Adaptation**\\n   - Over millions of years, parrots evolved colorful feathers to improve their chances of survival and reproduction in their specific environments. Natural selection favored birds whose colors helped them succeed in finding mates and avoiding predators.\\n\\nIn addition to these practical reasons, their striking appearance contributes to their popularity among humans!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 342, 'prompt_tokens': 15, 'total_tokens': 357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CedcQE1KO2AlRDgfqi80ZpChXBCIF', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1a9dfc77-6a39-48d5-82e5-d32f814d706c-0' usage_metadata={'input_tokens': 15, 'output_tokens': 342, 'total_tokens': 357, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content=\"Airplanes fly by generating lift, which counteracts gravity and allows the airplane to stay airborne. Four key forces act on an airplane during flight: **lift**, **weight (gravity)**, **thrust**, and **drag.** Here’s an outline of how airplanes fly:\\n\\n### 1. **Lift**\\nLift is generated by the airplane's wings as air flows over and under them. The shape of the wings, called an **airfoil**, is designed to create a pressure difference. \\n- As the airplane moves forward, air travels faster over the curved top surface of the wings, causing lower pressure compared to the slower-moving air under the wings, where pressure is higher. \\n- This pressure difference pushes the wings upward, creating lift.\\n\\n### 2. **Thrust**\\nThrust is produced by the airplane's engines, which propel the plane forward. This forward motion allows air to flow over the wings and generate lift. \\n- Jet engines or propellers create the force needed to overcome drag and maintain speed.\\n\\n### 3. **Weight (Gravity)**\\nWeight is the downward force caused by gravity acting on the airplane. Lift must be greater than or equal to weight for the airplane to become airborne and stay in flight.\\n\\n### 4. **Drag**\\nDrag is the resistance the airplane encounters as it moves through the air. The airplane's shape is designed to minimize drag. Engines must generate enough thrust to overcome drag for the airplane to keep moving forward.\\n\\n### The Role of the Pilot\\nThe pilot controls the airplane’s altitude, speed, and direction by adjusting various control surfaces like:\\n- **Elevators** (at the tail): control pitch (up/down movement of the nose).\\n- **Ailerons** (on the wings): control roll (tilting the wings to turn).\\n- **Rudder** (on the vertical stabilizer): controls yaw (side-to-side movement of the nose).\\n\\nBy managing these forces and controls effectively, airplanes are able to take off, cruise, and land safely.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 12, 'total_tokens': 420, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CedcQYmNqqSN0M3ubBRw5PYZbkzV5', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--26cc2d63-9921-4bba-a477-db65aeaf052a-0' usage_metadata={'input_tokens': 12, 'output_tokens': 408, 'total_tokens': 420, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content='Quantum computing is an advanced field of computer science and physics that leverages the principles of quantum mechanics to process and store information. Unlike classical computing, which uses bits as the fundamental unit of information (represented as 0 or 1), quantum computing uses **quantum bits** or **qubits**, which can exist in multiple states simultaneously thanks to quantum phenomena like **superposition** and **entanglement**.\\n\\nHere are key concepts that define quantum computing:\\n\\n1. **Superposition**: In classical computing, a bit can only be in one state at a time (0 or 1). A qubit, however, can exist in a superposition of both 0 and 1 simultaneously. This allows quantum computers to process a vast amount of possibilities at the same time, offering parallelism that has no classical equivalent.\\n\\n2. **Entanglement**: When qubits are entangled, they become correlated in such a way that the state of one qubit is dependent on the state of another, regardless of the physical distance between them. This phenomenon enables quantum computers to perform highly complex calculations and transmit information efficiently.\\n\\n3. **Quantum interference**: Quantum algorithms use interference to amplify the probabilities of correct solutions while canceling out incorrect ones, enabling faster computation for certain problems.\\n\\nQuantum computing holds great potential for solving problems that are intractable for classical computers, such as simulating complex molecular structures for drug development, optimizing large datasets, improving cryptography, and solving combinatorial optimization problems.\\n\\nHowever, practical quantum computing is still in its early stages due to challenges in scaling qubits, maintaining their coherence, and reducing errors caused by quantum noise. Leading organizations, such as IBM, Google, Intel, and startups like IonQ and Rigetti Computing, are actively working to develop more robust quantum technology. Popular frameworks for quantum programming include IBM’s Qiskit and Google’s Cirq.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 381, 'prompt_tokens': 12, 'total_tokens': 393, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CedcQDQe0ChLNAPm72GPxUbTdFRuX', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--e0d9fa9d-fa8b-4ee6-a19b-acf8c06d3885-0' usage_metadata={'input_tokens': 12, 'output_tokens': 381, 'total_tokens': 393, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Open Source LLMs with HuggingFace and LangChain"
      ],
      "metadata": {
        "id": "LhKalWAJrEx8"
      },
      "id": "LhKalWAJrEx8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing Open LLMs with HuggingFace Serverless API\n",
        "\n",
        "The free [serverless API](https://huggingface.co/inference-api/serverless) lets you implement solutions and iterate in no time, but it may be rate limited for heavy use cases, since the loads are shared with other requests.\n",
        "\n",
        "For enterprise workloads, you can use Inference Endpoints - Dedicated which would be hosted on a specific cloud instance of your choice and would have a cost associated with it. Here we will use the free serverless API which works quite well in most cases.\n",
        "\n",
        "The advantage is you do not need to download the models or run them locally on a GPU compute infrastructure which takes time and also would cost you a fair amount."
      ],
      "metadata": {
        "id": "6I-BRuFerKTv"
      },
      "id": "6I-BRuFerKTv"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-huggingface"
      ],
      "metadata": {
        "id": "D7lG9HADpPfK"
      },
      "id": "D7lG9HADpPfK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKOlP3l8Ctdj"
      },
      "id": "RKOlP3l8Ctdj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"]=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "I5qyiopoxGEX"
      },
      "id": "I5qyiopoxGEX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=userdata.get('HF_TOKEN'),\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"deepseek-ai/DeepSeek-V3.2-Exp:novita\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of France?\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr-nMxRVxfx9",
        "outputId": "154a3433-710f-45e8-9493-c9d8f2980c56"
      },
      "id": "Sr-nMxRVxfx9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='The capital of France is **Paris**.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-ai/DeepSeek-V3.2-Exp:novita\",  # Your Azure deployment name\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=userdata.get('HF_TOKEN')\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"Hello, how are you?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlhqfuX4pPOv",
        "outputId": "95518c20-5a95-40cf-a273-4f9f32cfe3d4"
      },
      "id": "dlhqfuX4pPOv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm doing great, thank you for asking! I'm here and ready to help you with whatever you need. How are you doing today? 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running LLM by downloading the Model to Local"
      ],
      "metadata": {
        "id": "tsgPRXbe30AV"
      },
      "id": "tsgPRXbe30AV"
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "# llm = HuggingFacePipeline.from_model_id(\n",
        "#     model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "#     task=\"text-generation\",\n",
        "#     pipeline_kwargs=dict(\n",
        "#         max_new_tokens=512,\n",
        "#         do_sample=False,\n",
        "#         repetition_penalty=1.03,\n",
        "#     ),\n",
        "# )\n",
        "\n",
        "# chat_model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "L15mNCUl3iQX"
      },
      "id": "L15mNCUl3iQX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.messages import (\n",
        "#     HumanMessage,\n",
        "#     SystemMessage,\n",
        "# )\n",
        "\n",
        "# messages = [\n",
        "#     SystemMessage(content=\"You're a helpful assistant\"),\n",
        "#     HumanMessage(\n",
        "#         content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "#     ),\n",
        "# ]\n",
        "\n",
        "# ai_msg = chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "8iENSiqh3iGl"
      },
      "id": "8iENSiqh3iGl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates\n",
        "Prompt templates are pre-designed formats used to generate prompts for language models. These templates can include instructions, few-shot examples, and specific contexts and questions suited for particular tasks.\n",
        "\n",
        "LangChain provides tools for creating and using prompt templates. It aims to develop model-agnostic templates to facilitate the reuse of existing templates across different language models. Typically, these models expect prompts in the form of either a string or a list of chat messages.\n",
        "\n",
        "### Types of Prompt Templates\n",
        "\n",
        "- **PromptTemplate:**\n",
        "  - Used for creating string-based prompts.\n",
        "  - Utilizes Python's `str.format` syntax for templating, supporting any number of variables, including scenarios with no variables.\n",
        "\n",
        "- **ChatPromptTemplate:**\n",
        "  - Designed for chat models, where the prompt consists of a list of chat messages.\n",
        "  - Each chat message includes content and a role parameter. For instance, in the OpenAI Chat Completions API, a chat message could be assigned to an AI assistant, a human, or a system role."
      ],
      "metadata": {
        "id": "-XIZlfbEy5KQ"
      },
      "id": "-XIZlfbEy5KQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Simple prompt\n",
        "\n",
        "prompt = \"\"\"Explain to me what is Generative AI in 3 bullet points?\"\"\"\n",
        "prompt_template = PromptTemplate.from_template(prompt)\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PEsCHBAylQ2",
        "outputId": "bcec95c2-d1b5-412f-ae9e-0685f4e25d84"
      },
      "id": "7PEsCHBAylQ2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Explain to me what is Generative AI in 3 bullet points?')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# more complex prompt with placeholders\n",
        "prompt = \"\"\"Explain to me briefly about {topic} in {language}.\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(prompt)\n",
        "prompt_template"
      ],
      "metadata": {
        "id": "ilO0E75F3iAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de0a325-627a-462c-fdef-9521586c3c3d"
      },
      "id": "ilO0E75F3iAV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['language', 'topic'], input_types={}, partial_variables={}, template='Explain to me briefly about {topic} in {language}.')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# simple prompt with placeholders\n",
        "prompt = \"\"\"Explain to me briefly about {topic}.\"\"\"\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_template(prompt)\n",
        "chat_template"
      ],
      "metadata": {
        "id": "9dJZxX_B3h4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ecba97-d9c7-48c6-f0fb-32ee1b920646"
      },
      "id": "9dJZxX_B3h4N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain to me briefly about {topic}.'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template.messages[0]"
      ],
      "metadata": {
        "id": "4EeCGnrd3ha-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c355071-5c9a-4722-97dd-bdfdf2b3bf75"
      },
      "id": "4EeCGnrd3ha-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain to me briefly about {topic}.'), additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial prompt templates\n",
        "\n",
        "Like other methods, it can make sense to \"partial\" a prompt template - e.g. pass in a subset of the required data values, as to create a new prompt template which expects only the remaining subset of data values.\n",
        "\n",
        "Imagine you get some inputs initially and then some other inputs for the prompts later, you can use the `partial`function to add some of the input data first and the rest of the input data later"
      ],
      "metadata": {
        "id": "0cRUhLdJ4OtN"
      },
      "id": "0cRUhLdJ4OtN"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def _get_datetime():\n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%m/%d/%Y\")"
      ],
      "metadata": {
        "id": "DJHbvQIV4Ujg"
      },
      "id": "DJHbvQIV4Ujg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_txt = \"\"\"Tell me a joke about {topic} on the day {date}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_txt)"
      ],
      "metadata": {
        "id": "qfNnUBoF4WsI"
      },
      "id": "qfNnUBoF4WsI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt7vwX7l4Z86",
        "outputId": "8ff4ecf0-f89d-4387-9163-9a62b8f599eb"
      },
      "id": "vt7vwX7l4Z86",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['date', 'topic'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['date', 'topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic} on the day {date}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt.partial(date=_get_datetime)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3g5etM4ftk",
        "outputId": "bfd27b97-1815-450a-ae96-d994fb034e4b"
      },
      "id": "Br3g5etM4ftk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={'date': <function _get_datetime at 0x7bbcb5f6a5c0>}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['date', 'topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic} on the day {date}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topics = ['Engineers', 'Statisticians']\n",
        "final_prompts = [prompt.format(topic=topic) for topic in topics]\n",
        "final_prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebd7nxj74rJA",
        "outputId": "ccbd11b8-758f-4b48-adcd-db9c0140ce7a"
      },
      "id": "ebd7nxj74rJA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Human: Tell me a joke about Engineers on the day 11/22/2025',\n",
              " 'Human: Tell me a joke about Statisticians on the day 11/22/2025']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using from list of tuples where each tuple container system,user and ai roles along with its corresponding message"
      ],
      "metadata": {
        "id": "U_-qmINs1WWy"
      },
      "id": "U_-qmINs1WWy"
    },
    {
      "cell_type": "code",
      "source": [
        "# simple prompt with placeholders using from_messages\n",
        "messages = [(\"system\",\"you are an expert in Gen AI\"),(\"human\", \"Explain to me briefly about {topic}.\")]\n",
        "\n",
        "chat_template1 = ChatPromptTemplate.from_messages(messages)\n",
        "chat_template1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwoNsCvX0Hpk",
        "outputId": "49fe5e18-5062-4fcb-eb38-2d4509374044"
      },
      "id": "TwoNsCvX0Hpk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert in Gen AI'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain to me briefly about {topic}.'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template1.messages[1].format(topic='english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPOIWw5A0ND4",
        "outputId": "55315ffb-1e75-47cc-f63d-473a9a193b90"
      },
      "id": "BPOIWw5A0ND4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content='Explain to me briefly about english.', additional_kwargs={}, response_metadata={})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_messages=[('system','you are an expert in Gen AI'),('user','Explain to me briefly about {topic}.')]\n"
      ],
      "metadata": {
        "id": "Jq3gPYRg0t8y"
      },
      "id": "Jq3gPYRg0t8y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChatPromptTemplate.from_messages(list_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfpyYgkt1CA2",
        "outputId": "da8d51d2-7c1a-41d6-8349-52481b03af00"
      },
      "id": "RfpyYgkt1CA2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert in Gen AI'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain to me briefly about {topic}.'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, **runnables** are a core protocol and abstraction for building, executing, and composing chains of tasks, models, or functions. Runnables enable you to sequence, parallelize, and manage LLM workflows with consistent interfaces.\n",
        "\n",
        "### What is a Runnable?\n",
        "- A runnable wraps any callable function, LLM, or workflow step so it can be invoked, piped, batched, or streamed.\n",
        "- You use runnables to construct chains, where the output of one runnable becomes the input to the next, enabling both simple and complex pipelines.\n",
        "- All runnables provide key methods, including:\n",
        "  - `.invoke(input)`: runs the callable with a single input.\n",
        "  - `.batch([inputs])`: runs the callable over a list of inputs, often in parallel.\n",
        "  - `.stream(input)`: streams the output, useful for LLMs and chatbots.\n",
        "\n",
        "### Common Runnable Types\n",
        "- **RunnableLambda:** Wraps any Python function into a runnable object.\n",
        "- **RunnableSequence:** Chains multiple runnables in sequence, passing output from one to the next.\n",
        "- **RunnableParallel:** Runs multiple runnables in parallel and gathers outputs.\n",
        "- **RunnablePassthrough:** Passes data unchanged, often for compatibility or branching tasks.\n",
        "- **RouterRunnable:** Routes input to other runnables depending on input type or keys\n",
        "\n",
        "\n",
        "### Benefits\n",
        "- Lets you build, extend, and debug composite ML and LLM workflows modularly.\n",
        "- Brings functional programming concepts (map, pipe, reduce) to agentic AI and LLM operations.\n",
        "- Runnables are the backbone of LangChain agent, tool, and chain orchestration, making code more extensible and maintainable.\n",
        "\n",
        "In summary: **Runnables are reusable, composable workflow units in LangChain for creating, executing, and chaining LLM-powered automation**\n"
      ],
      "metadata": {
        "id": "wFdp4zzf98pr"
      },
      "id": "wFdp4zzf98pr"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def greet(name):\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "greet_runnable = RunnableLambda(lambda x: greet(x))\n",
        "result = greet_runnable.invoke(\"Alice\")    # Output: \"Hello, Alice!\"\n"
      ],
      "metadata": {
        "id": "Tbr5L5Vt6k1B"
      },
      "id": "Tbr5L5Vt6k1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from annotated_types import UpperCase\n",
        "def count_chars(text):\n",
        "    return text.upper()\n",
        "\n",
        "count_chars_runnable = RunnableLambda(count_chars)"
      ],
      "metadata": {
        "id": "qn8-lGJz64VK"
      },
      "id": "qn8-lGJz64VK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_chars_runnable.invoke(\"Mahesh!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CksbETnE7TcA",
        "outputId": "7c72deac-2fc1-4fdc-c7a1-25646ba40651"
      },
      "id": "CksbETnE7TcA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MAHESH!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableSequence"
      ],
      "metadata": {
        "id": "2fAI2Ig17fKq"
      },
      "id": "2fAI2Ig17fKq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_length = RunnableSequence(first=greet_runnable,last=count_chars_runnable)"
      ],
      "metadata": {
        "id": "KR2ypMmZ7mIv"
      },
      "id": "KR2ypMmZ7mIv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_length.invoke('Mahesh!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y8dpId6R7-RH",
        "outputId": "fab398d1-3327-4b41-af61-35d6afedf254"
      },
      "id": "y8dpId6R7-RH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HELLO, MAHESH!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_length1 = greet_runnable | count_chars_runnable"
      ],
      "metadata": {
        "id": "b7PgVp_V8L-V"
      },
      "id": "b7PgVp_V8L-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_length1.invoke('Mahesh')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ed4ad_Nj8Tw9",
        "outputId": "631561b0-d667-4b7b-e768-2cad4c200a9e"
      },
      "id": "Ed4ad_Nj8Tw9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HELLO, MAHESH!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers"
      ],
      "metadata": {
        "id": "ueBWIJOb5cAR"
      },
      "id": "ueBWIJOb5cAR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728f2eee",
      "metadata": {
        "id": "728f2eee"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser # pyright: ignore[reportMissingImports]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d574325f",
      "metadata": {
        "id": "d574325f"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate.from_template(\"What is {subject}?. explain in 3 bullet points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31065d0a",
      "metadata": {
        "id": "31065d0a"
      },
      "outputs": [],
      "source": [
        "final_prompt=prompt.invoke({\"subject\": \"LangChain\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2e3d92",
      "metadata": {
        "id": "cf2e3d92"
      },
      "outputs": [],
      "source": [
        "reponse=llm.invoke(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e03c613",
      "metadata": {
        "id": "2e03c613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fef4cd-5ad3-41f4-c02f-89564ca12a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='- **Modular Framework**: LangChain is a versatile framework designed to streamline the development of applications using language models. It provides a structured approach to build complex applications that can handle tasks such as information retrieval, language generation, and conversational interactions.\\n\\n- **Integration and Extensibility**: The framework is built to easily integrate with various data sources, APIs, and machine learning models. This allows developers to extend the capabilities of language models by combining them with other technologies and datasets, leading to more robust and feature-rich applications.\\n\\n- **Focus on Agent-based Systems**: LangChain emphasizes the creation of agent-based systems that can reason, act, and interact based on inputs. By using chains and agents, developers can construct sophisticated workflows where language models are embedded within larger logic-driven environments, enabling nuanced decision-making processes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 163, 'prompt_tokens': 18, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CeecPaSPaYK8Qg4rJKlfZxTa5umOk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2f2c9f73-364e-4574-8087-652dcea02e50-0', usage_metadata={'input_tokens': 18, 'output_tokens': 163, 'total_tokens': 181, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "StrOutputParser().parse(reponse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9475df7f",
      "metadata": {
        "id": "9475df7f"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "R4Fs_GIq_gG9",
        "outputId": "cc261a9a-506c-40e0-8e78-054f6e5af1f1"
      },
      "id": "R4Fs_GIq_gG9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableSequence"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableSequence</b><br/>def __init__(*steps: RunnableLike, name: str | None=None, first: Runnable[Any, Any] | None=None, middle: list[Runnable[Any, Any]] | None=None, last: Runnable[Any, Any] | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py</a>Sequence of `Runnable` objects, where the output of one is the input of the next.\n",
              "\n",
              "**`RunnableSequence`** is the most important composition operator in LangChain\n",
              "as it is used in virtually every chain.\n",
              "\n",
              "A `RunnableSequence` can be instantiated directly or more commonly by using the\n",
              "`|` operator where either the left or right operands (or both) must be a\n",
              "`Runnable`.\n",
              "\n",
              "Any `RunnableSequence` automatically supports sync, async, batch.\n",
              "\n",
              "The default implementations of `batch` and `abatch` utilize threadpools and\n",
              "asyncio gather and will be faster than naive invocation of `invoke` or `ainvoke`\n",
              "for IO bound `Runnable`s.\n",
              "\n",
              "Batching is implemented by invoking the batch method on each component of the\n",
              "`RunnableSequence` in order.\n",
              "\n",
              "A `RunnableSequence` preserves the streaming properties of its components, so if\n",
              "all components of the sequence implement a `transform` method -- which\n",
              "is the method that implements the logic to map a streaming input to a streaming\n",
              "output -- then the sequence will be able to stream input to output!\n",
              "\n",
              "If any component of the sequence does not implement transform then the\n",
              "streaming will only begin after this component is run. If there are\n",
              "multiple blocking components, streaming begins after the last one.\n",
              "\n",
              "!!! note\n",
              "    `RunnableLambdas` do not support `transform` by default! So if you need to\n",
              "    use a `RunnableLambdas` be careful about where you place them in a\n",
              "    `RunnableSequence` (if you need to use the `stream`/`astream` methods).\n",
              "\n",
              "    If you need arbitrary logic and need streaming, you can subclass\n",
              "    Runnable, and implement `transform` for whatever logic you need.\n",
              "\n",
              "Here is a simple example that uses simple functions to illustrate the use of\n",
              "`RunnableSequence`:\n",
              "\n",
              "    ```python\n",
              "    from langchain_core.runnables import RunnableLambda\n",
              "\n",
              "\n",
              "    def add_one(x: int) -&gt; int:\n",
              "        return x + 1\n",
              "\n",
              "\n",
              "    def mul_two(x: int) -&gt; int:\n",
              "        return x * 2\n",
              "\n",
              "\n",
              "    runnable_1 = RunnableLambda(add_one)\n",
              "    runnable_2 = RunnableLambda(mul_two)\n",
              "    sequence = runnable_1 | runnable_2\n",
              "    # Or equivalently:\n",
              "    # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
              "    sequence.invoke(1)\n",
              "    await sequence.ainvoke(1)\n",
              "\n",
              "    sequence.batch([1, 2, 3])\n",
              "    await sequence.abatch([1, 2, 3])\n",
              "    ```\n",
              "\n",
              "Here&#x27;s an example that uses streams JSON output generated by an LLM:\n",
              "\n",
              "    ```python\n",
              "    from langchain_core.output_parsers.json import SimpleJsonOutputParser\n",
              "    from langchain_openai import ChatOpenAI\n",
              "\n",
              "    prompt = PromptTemplate.from_template(\n",
              "        &quot;In JSON format, give me a list of {topic} and their &quot;\n",
              "        &quot;corresponding names in French, Spanish and in a &quot;\n",
              "        &quot;Cat Language.&quot;\n",
              "    )\n",
              "\n",
              "    model = ChatOpenAI()\n",
              "    chain = prompt | model | SimpleJsonOutputParser()\n",
              "\n",
              "    async for chunk in chain.astream({&quot;topic&quot;: &quot;colors&quot;}):\n",
              "        print(&quot;-&quot;)  # noqa: T201\n",
              "        print(chunk, sep=&quot;&quot;, flush=True)  # noqa: T201\n",
              "    ```</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2789);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea725bc8",
      "metadata": {
        "id": "ea725bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7ba9bd-d346-4baf-ad66-de98fb463856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Framework for Language Model Applications**: LangChain is a framework designed to streamline the development of applications powered by large language models (LLMs). It provides a structured way to build complex applications, enabling seamless integration of various components required for sophisticated language model operations.\n",
            "\n",
            "- **Composability and Modularity**: By promoting a composable approach, LangChain allows developers to modularly combine different functionalities, such as prompt engineering, chaining of multiple language models, memory management, and interaction with various data sources. This enables the creation of more adaptable and efficient language-based systems.\n",
            "\n",
            "- **Enhanced Capabilities for Advanced Use Cases**: It offers tools for creating advanced use cases, such as chatbots, data augmentation, dynamic document retrieval, and more. By facilitating interaction with external systems and incorporating memory and state, LangChain supports the development of applications that can go beyond simple text generation to achieve more context-aware and data-driven functionalities.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke({\"subject\": \"LangChain\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac752341",
      "metadata": {
        "id": "ac752341"
      },
      "outputs": [],
      "source": [
        "list_topics = [\"Artificial Intelligence\", \"Machine Learning\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed0e8f1",
      "metadata": {
        "id": "0ed0e8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c505ad2b-8e1b-400d-96d2-b899e67076d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"- **Definition and Functionality**: Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions), and self-correction.\\n\\n- **Subfields and Techniques**: AI encompasses several subfields which include machine learning, where algorithms are used to find patterns in data, and neural networks, which are inspired by the human brain's network of neurons. AI also involves techniques like natural language processing, allowing machines to understand and respond to human language, and computer vision, which enables machines to interpret and process visual information.\\n\\n- **Applications and Impacts**: AI is utilized in a wide range of applications such as autonomous vehicles, personalized recommendations on streaming services, fraud detection, and virtual assistants like Siri and Alexa. Its impact extends across various industries, enhancing efficiency, creating new opportunities, and posing ethical challenges regarding privacy, employment, and decision-making transparency.\",\n",
              " \"- **Definition and Purpose**: Machine Learning is a subset of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to perform specific tasks without using explicit instructions, relying on patterns and inference instead. It's primarily used to improve decision-making and predictions by learning from data.\\n\\n- **Learning from Data**: At its core, Machine Learning involves training a model on data to recognize patterns, trends, and relationships. This training process allows the model to generalize from the observed examples to new, unseen situations, ultimately improving its performance on tasks like classification, regression, and clustering.\\n\\n- **Applications and Impact**: Machine Learning is widely applied across various industries, including finance, healthcare, marketing, and technology. It powers applications such as recommendation systems, fraud detection, image and speech recognition, and autonomous vehicles, significantly impacting the way businesses operate and innovate.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "chain.map().invoke(list_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers\n",
        "Output parsers are essential in Langchain for structuring the responses from language models. Below, we will discuss the role of output parsers and include examples using Langchain's specific parser types: PydanticOutputParser, String, and CommaSeparatedListOutputParser.\n",
        "\n",
        "- **String output parser:**\n",
        "  - This parser allows the specification of an arbitrary Pydantic Model to query LLMs for outputs matching that schema. Pydantic's BaseModel functions similarly to a Python dataclass but includes type checking and coercion.\n",
        "\n",
        "- **PydanticOutputParser:**\n",
        "  - Users can specify an arbitrary JSON schema with this parser to ensure outputs from LLMs adhere to that schema. Pydantic can also be used to declare your data model here.\n",
        "\n",
        "- **CSV parser:**\n",
        "  - Useful for outputs requiring a list of items separated by commas. This parser facilitates the extraction of comma-separated values from model outputs.\n"
      ],
      "metadata": {
        "id": "WYmdORlwFUz6"
      },
      "id": "WYmdORlwFUz6"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\",api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "reponse=llm.invoke(\"explain about GenAI briefly\")"
      ],
      "metadata": {
        "id": "Qb9bYfBfJRyj"
      },
      "id": "Qb9bYfBfJRyj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reponse.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "AivhmePBMl6j",
        "outputId": "069eac10-22eb-49f9-c67b-6c64264e8ef9"
      },
      "id": "AivhmePBMl6j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Of course! Here is a brief explanation of GenAI.\\n\\n**GenAI (Generative Artificial Intelligence)** is a type of AI that can **create new, original content**, rather than just analyzing or acting on existing data.\\n\\nThink of it like this:\\n*   **Traditional AI** is like a detective. It analyzes clues (data) to find a specific answer or identify something (e.g., \"Is this a cat or a dog?\").\\n*   **Generative AI** is like an artist or an author. It learns from a vast library of examples and then uses that knowledge to create a brand-new painting, story, or piece of music.\\n\\n**How it Works (Simply):**\\nIt\\'s trained on massive amounts of data (like text, images, or code from the internet). It learns the patterns, styles, and structures within that data. When you give it a prompt (a command or question), it uses its training to generate something completely new that fits your request.\\n\\n**What it Can Create:**\\n*   **Text:** Essays, emails, poems, and computer code.\\n*   **Images:** Photorealistic scenes, logos, and artistic images from a simple description.\\n*   **Audio:** Music, realistic voiceovers, and sound effects.\\n*   **Video:** Short video clips and animations.\\n\\n**Famous Examples:**\\n*   **ChatGPT:** For generating text.\\n*   **Midjourney & DALL-E 3:** For creating images from text.\\n*   **Google Gemini:** A multimodal model that works with text, images, and code.\\n\\nIn short, GenAI is a creative partner that generates new things, making it a powerful tool for boosting creativity, productivity, and problem-solving.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser,PydanticOutputParser,CommaSeparatedListOutputParser"
      ],
      "metadata": {
        "id": "4QF3csoGJJzt"
      },
      "id": "4QF3csoGJJzt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template('Explain about the {topic} briefly?')"
      ],
      "metadata": {
        "id": "XgC5eEHhL_Fh"
      },
      "id": "XgC5eEHhL_Fh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "_zXf8dVIMMqD"
      },
      "id": "_zXf8dVIMMqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_response = chain.invoke({\"topic\": \"GenAI\"})"
      ],
      "metadata": {
        "id": "J-j_ULvKMkDO"
      },
      "id": "J-j_ULvKMkDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "eeKHdtT5M8Vc",
        "outputId": "a8e35888-a2cb-4389-a04d-6aa58ac93425"
      },
      "id": "eeKHdtT5M8Vc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Of course! Here is a brief explanation of Generative AI.\\n\\n---\\n\\n### What is Generative AI (GenAI)?\\n\\nIn simple terms, **Generative AI is a type of artificial intelligence that can create new, original content.**\\n\\nInstead of just analyzing or classifying existing data, GenAI learns patterns from massive amounts of information (like text, images, or code) and then uses that knowledge to generate something entirely new.\\n\\n---\\n\\n### A Simple Analogy\\n\\nThink of it like a student of music. A traditional AI might be able to listen to a song and tell you if it's classical or jazz (classification).\\n\\nA **Generative AI** is like a student who has listened to thousands of classical songs, learned the rules, styles, and structures, and can now **compose a brand new classical melody** that has never been heard before.\\n\\n---\\n\\n### What Can It Create?\\n\\nGenAI can produce a wide variety of content, including:\\n\\n*   **Text:** Writing emails, articles, stories, and computer code (e.g., **ChatGPT**, **Google Gemini**).\\n*   **Images:** Creating realistic photos, art, and logos from a simple text description (e.g., **DALL-E**, **Midjourney**).\\n*   **Audio:** Composing music, generating realistic voiceovers, and creating sound effects.\\n*   **Video:** Generating or editing video clips.\\n\\n### Why is it a Big Deal?\\n\\nGenAI is a major breakthrough because it shifts AI from being a tool for **understanding** the world to a tool for **creating** within it. It acts as a powerful assistant that can brainstorm ideas, automate creative tasks, and help solve complex problems in a more human-like way.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "KKTQsU5NNeg9"
      },
      "id": "KKTQsU5NNeg9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class pyformatter(BaseModel):\n",
        "  Details : str = Field(description=\"explain about the topic\")\n",
        "  pros : List[str] = Field(description=\"pros of the topic\")\n",
        "  cons : List[str] = Field(description=\"pros of the topic\")"
      ],
      "metadata": {
        "id": "0awWt-VpNjr8"
      },
      "id": "0awWt-VpNjr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser=PydanticOutputParser(pydantic_object=pyformatter)"
      ],
      "metadata": {
        "id": "EH3j-UTXOMO5"
      },
      "id": "EH3j-UTXOMO5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.get_input_schema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "GFScV1Ohbv0f",
        "outputId": "5f2a9733-e782-432f-b730-27027dea3192"
      },
      "id": "GFScV1Ohbv0f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.output_parsers.pydantic.PydanticOutputParserInput"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.output_parsers.pydantic.PydanticOutputParserInput</b><br/>def __init__(self, /, root: RootModelRootType=PydanticUndefined, **data) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/output_parsers/pydantic.py</a>!!! abstract &quot;Usage Documentation&quot;\n",
              "    [`RootModel` and Custom Root Types](../concepts/models.md#rootmodel-and-custom-root-types)\n",
              "\n",
              "A Pydantic `BaseModel` for the root object of the model.\n",
              "\n",
              "Attributes:\n",
              "    root: The root object of the model.\n",
              "    __pydantic_root_model__: Whether the model is a RootModel.\n",
              "    __pydantic_private__: Private fields in the model.\n",
              "    __pydantic_extra__: Extra fields in the model.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, None);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.get_format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "cKGnqabQOn6H",
        "outputId": "4f3732dd-7187-48f3-bf25-90f095d266dc"
      },
      "id": "cKGnqabQOn6H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"Details\": {\"description\": \"explain about the topic\", \"title\": \"Details\", \"type\": \"string\"}, \"pros\": {\"description\": \"pros of the topic\", \"items\": {\"type\": \"string\"}, \"title\": \"Pros\", \"type\": \"array\"}, \"cons\": {\"description\": \"pros of the topic\", \"items\": {\"type\": \"string\"}, \"title\": \"Cons\", \"type\": \"array\"}}, \"required\": [\"Details\", \"pros\", \"cons\"]}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_prompt = ChatPromptTemplate.from_template('Explain about the {topic} briefly?{format_instructions}')"
      ],
      "metadata": {
        "id": "NWj-Hb5iQS1J"
      },
      "id": "NWj-Hb5iQS1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = full_prompt | llm | parser"
      ],
      "metadata": {
        "id": "I5YWEgziPUXk"
      },
      "id": "I5YWEgziPUXk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reponse1 = chain.invoke({'topic': 'GenAI', 'format_instructions': parser.get_format_instructions()})"
      ],
      "metadata": {
        "id": "-QSEXpALPk_B"
      },
      "id": "-QSEXpALPk_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reponse1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54_zS7mgPsDd",
        "outputId": "0edb1372-a9ed-420e-aeb2-e02365021eed"
      },
      "id": "54_zS7mgPsDd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Details='Generative AI (GenAI) is a type of artificial intelligence that can create new, original content, such as text, images, music, and code. Unlike traditional AI that analyzes or classifies existing data, GenAI learns patterns and structures from vast datasets and then uses that knowledge to generate novel outputs that mimic human creativity. It is powered by complex models like Large Language Models (LLMs) and diffusion models.' pros=['Boosts productivity and automates content creation across various fields.', 'Enhances human creativity by acting as a brainstorming partner and idea generator.', 'Enables hyper-personalization of services and products at scale.', 'Accelerates research and development in complex areas like drug discovery and engineering.', 'Democratizes skills like coding, writing, and design, making them more accessible.'] cons=[\"Can generate inaccurate or nonsensical information, known as 'hallucinations'.\", 'May inherit and amplify biases present in the training data, leading to unfair outputs.', 'Raises significant ethical concerns regarding misuse, such as creating deepfakes and spreading misinformation.', 'Issues with copyright and intellectual property ownership of AI-generated content.', 'Requires immense computational power and energy, leading to high costs and environmental impact.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RheDTIJXVGzX"
      },
      "id": "RheDTIJXVGzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(reponse1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "CAoW5RLAVJ-x",
        "outputId": "8bc690fe-25ee-4d93-b34a-85fa8ca394fe"
      },
      "id": "CAoW5RLAVJ-x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0                                                  1\n",
              "0  Details  Generative AI (GenAI) is a type of artificial ...\n",
              "1     pros  [Boosts productivity and automates content cre...\n",
              "2     cons  [Can generate inaccurate or nonsensical inform..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52167801-0fbb-4a98-9531-8069969cbb51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Details</td>\n",
              "      <td>Generative AI (GenAI) is a type of artificial ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pros</td>\n",
              "      <td>[Boosts productivity and automates content cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cons</td>\n",
              "      <td>[Can generate inaccurate or nonsensical inform...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52167801-0fbb-4a98-9531-8069969cbb51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52167801-0fbb-4a98-9531-8069969cbb51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52167801-0fbb-4a98-9531-8069969cbb51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-026cd242-572b-421d-a87b-8882725aa06e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-026cd242-572b-421d-a87b-8882725aa06e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-026cd242-572b-421d-a87b-8882725aa06e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Details\",\n          \"pros\",\n          \"cons\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "COHaMmd6Q8zJ",
        "outputId": "ef3aa5e6-2cf5-4888-b428-443cf5a7ce4e"
      },
      "id": "COHaMmd6Q8zJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# And a query intented to prompt a language model to populate the data structure.\n",
        "prompt_txt = \"\"\"\n",
        "             Create a list of 5 different ways in which Generative AI can be used\n",
        "\n",
        "             Output format instructions:\n",
        "             {format_instructions}\n",
        "             \"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template=prompt_txt)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVjSY9GCQ_tY",
        "outputId": "ba815505-9a0d-4d4e-ee25-595740f85aa3"
      },
      "id": "KVjSY9GCQ_tY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['format_instructions'], input_types={}, partial_variables={}, template='\\n             Create a list of 5 different ways in which Generative AI can be used\\n\\n             Output format instructions:\\n             {format_instructions}\\n             ')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple LLM Chain - more on this later\n",
        "llm_chain = (prompt\n",
        "              |\n",
        "            llm\n",
        "              |\n",
        "            output_parser)\n",
        "\n",
        "# run the chain\n",
        "response = llm_chain.invoke({'format_instructions': format_instructions})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUabSl9UVkpw",
        "outputId": "50080ea1-95b9-47e3-aa4c-1143e87498e7"
      },
      "id": "bUabSl9UVkpw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Creative Content Generation',\n",
              " 'Code Generation and Debugging',\n",
              " 'Scientific Discovery and Research',\n",
              " 'Personalized Education and Tutoring',\n",
              " 'Synthetic Data Creation for AI Training']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3uPQ0uyW6oZ"
      },
      "id": "a3uPQ0uyW6oZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class csv_fomatter(BaseModel):\n",
        "  uses : List[str] = Field(description=\"give the list of ways how the topic can be used\")"
      ],
      "metadata": {
        "id": "l5gIJ_uaW6lJ"
      },
      "id": "l5gIJ_uaW6lJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(csv_fomatter,method='json_schema')"
      ],
      "metadata": {
        "id": "6Ri6haO4VndQ"
      },
      "id": "6Ri6haO4VndQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_prompt=ChatPromptTemplate.from_template('Create a list of 5 different ways in which {topic} can be used')"
      ],
      "metadata": {
        "id": "1RCmP_PQX2oV"
      },
      "id": "1RCmP_PQX2oV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain2=csv_prompt|structured_llm"
      ],
      "metadata": {
        "id": "g9Q3CZ5nW8qN"
      },
      "id": "g9Q3CZ5nW8qN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2=chain2.invoke({'topic': 'Gen AI'})"
      ],
      "metadata": {
        "id": "iqx2waIiXNf2"
      },
      "id": "iqx2waIiXNf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2.uses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcKXE-vsXOcZ",
        "outputId": "7eb8db86-2625-4941-9e1c-e708323ec50f"
      },
      "id": "CcKXE-vsXOcZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Generating creative content such as articles, music, and images.',\n",
              " 'Assisting in software development by writing, debugging, and optimizing code.',\n",
              " 'Powering advanced chatbots and virtual assistants for customer service.',\n",
              " 'Accelerating scientific research by analyzing data and simulating complex models.',\n",
              " 'Creating personalized educational materials and tutoring systems.']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3=chain2.invoke({'topic': 'Agnetic AI'})"
      ],
      "metadata": {
        "id": "o14eZI2sYJvV"
      },
      "id": "o14eZI2sYJvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-SFVF-3aDB8",
        "outputId": "ec2f2f75-82e0-4ede-f5df-e7e7d091dda5"
      },
      "id": "X-SFVF-3aDB8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "csv_fomatter(uses=['Automated Software Development: AI agents can be tasked with creating entire applications from a simple prompt, handling everything from writing code and debugging to testing and deployment.', \"Proactive Personal Assistants: An agentic AI can manage a user's calendar, book appointments, plan travel, and even respond to emails based on learned preferences and goals, going beyond simple command execution.\", 'Scientific Research and Discovery: These AI systems can be tasked with a research goal, where they autonomously formulate hypotheses, sift through vast datasets, design experiments, and analyze results to accelerate scientific breakthroughs.', 'Autonomous Business Process Management: Agentic AI can manage complex workflows like supply chain logistics, financial auditing, or customer onboarding by making independent decisions, interacting with different systems, and resolving issues without human intervention.', 'Cybersecurity Defense: An AI agent can act as an autonomous security analyst, constantly monitoring networks for threats, identifying vulnerabilities, and actively neutralizing attacks in real-time.'])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3.uses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_aZnQZ-Ysd9",
        "outputId": "89778ff1-04d9-4bb2-a27e-bb2c4ad326df"
      },
      "id": "h_aZnQZ-Ysd9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Automated Software Development: AI agents can be tasked with creating entire applications from a simple prompt, handling everything from writing code and debugging to testing and deployment.',\n",
              " \"Proactive Personal Assistants: An agentic AI can manage a user's calendar, book appointments, plan travel, and even respond to emails based on learned preferences and goals, going beyond simple command execution.\",\n",
              " 'Scientific Research and Discovery: These AI systems can be tasked with a research goal, where they autonomously formulate hypotheses, sift through vast datasets, design experiments, and analyze results to accelerate scientific breakthroughs.',\n",
              " 'Autonomous Business Process Management: Agentic AI can manage complex workflows like supply chain logistics, financial auditing, or customer onboarding by making independent decisions, interacting with different systems, and resolving issues without human intervention.',\n",
              " 'Cybersecurity Defense: An AI agent can act as an autonomous security analyst, constantly monitoring networks for threats, identifying vulnerabilities, and actively neutralizing attacks in real-time.']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"question\": {\"type\": \"string\", \"description\": \"The trivia question\"},\n",
        "        \"answer\": {\"type\": \"string\", \"description\": \"The correct answer\"}\n",
        "    },\n",
        "    \"required\": [\"question\", \"answer\"]\n",
        "}\n",
        "\n",
        "structured_llm = llm.with_structured_output(json_schema,method=\"json_schema\")\n",
        "\n",
        "result = structured_llm.invoke(\"Give me 10 questions on langchain runnables,output parsers and models in langchain\")\n",
        "print(result)  # returns structured dict matching the schema\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxJCWrhmZr5A",
        "outputId": "3ce41891-dac9-450f-ee7d-ce1babca30cc"
      },
      "id": "uxJCWrhmZr5A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'What is the primary protocol that all components in the LangChain Expression Language (LCEL) must implement, providing methods like `invoke` and `batch`?', 'answer': 'The Runnable protocol'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysbEtmx-aSA1",
        "outputId": "7e7c35b6-2004-488a-b2fc-8d5910a3e9d0"
      },
      "id": "ysbEtmx-aSA1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.with_structured_output()['function_calling', 'json_mode', 'json_schema']"
      ],
      "metadata": {
        "id": "_ABEbm5lZ_q9"
      },
      "id": "_ABEbm5lZ_q9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJgNF2EGbqm1"
      },
      "id": "vJgNF2EGbqm1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}