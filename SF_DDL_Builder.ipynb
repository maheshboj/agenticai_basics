{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEE3qHIp8jUi+S/kaiiqHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshboj/agenticai_basics/blob/Langchain_components/SF_DDL_Builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJcW0_n3Mlx",
        "outputId": "1cc38c75-5e7f-42d9-ed07-ddc8b7a551b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement lanchain_openai (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for lanchain_openai\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install langchain lanchain_openai pyarrow pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S_JvtRWVFmG",
        "outputId": "1e2907c2-bcd1-4fd6-8b51-150c0be3a606"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "hCTRKTh8Frtk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_community"
      ],
      "metadata": {
        "id": "fwQrBsSGUX2p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU snowflake-connector-python snowflake-sqlalchemy"
      ],
      "metadata": {
        "id": "RnYWi_z-UdWA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "from langchain.agents import create_agent\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.tools import tool\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import yaml\n",
        "from langchain_community.utilities import SQLDatabase"
      ],
      "metadata": {
        "id": "WJyihbw33rJC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_schema(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Convert pandas DataFrame dtypes to a simple schema representation:\n",
        "    [{ \"column\": \"id\", \"dtype\": \"int64\", \"nullable\": False, \"sample_values\": [...] }, ...]\n",
        "    \"\"\"\n",
        "    schema = []\n",
        "    # small samples for inference\n",
        "    sample = df.head(10).to_dict(orient=\"list\")\n",
        "    for col in df.columns:\n",
        "        dtype = str(df[col].dtype)\n",
        "        nullable = df[col].isnull().any()\n",
        "        samples = sample.get(col, [])[:5]\n",
        "        schema.append({\n",
        "            \"column\": col,\n",
        "            \"dtype\": dtype,\n",
        "            \"nullable\": bool(nullable),\n",
        "            \"sample_values\": samples\n",
        "        })\n",
        "    return schema"
      ],
      "metadata": {
        "id": "_XNuast13vHe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"read_local_folder_and_infer_schema\")\n",
        "def read_local_folder_and_infer_schema(folder_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Scans a folder, reads supported files (csv, json, parquet, xlsx),\n",
        "    and returns a JSON string that lists each file and its inferred schema.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for entry in os.listdir(folder_path):\n",
        "        full = os.path.join(folder_path, entry)\n",
        "        if os.path.isdir(full):\n",
        "            continue\n",
        "        name_lower = entry.lower()\n",
        "        try:\n",
        "            if name_lower.endswith(\".csv\"):\n",
        "                df = pd.read_csv(full, nrows=1000)  # sample up to 1000 rows\n",
        "            elif name_lower.endswith(\".json\"):\n",
        "                # Try JSON lines first, fallback to standard JSON\n",
        "                try:\n",
        "                    df = pd.read_json(full, lines=True)\n",
        "                except ValueError:\n",
        "                    df = pd.read_json(full)\n",
        "            elif name_lower.endswith(\".parquet\") or name_lower.endswith(\".pq\"):\n",
        "                df = pq.read_table(full).to_pandas()\n",
        "            elif name_lower.endswith(\".xlsx\") or name_lower.endswith(\".xls\"):\n",
        "                df = pd.read_excel(full)\n",
        "            else:\n",
        "                # skip unknown\n",
        "                continue\n",
        "        except Exception as e:\n",
        "            result.append({\n",
        "                \"file\": entry,\n",
        "                \"error\": f\"failed to read: {repr(e)}\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        schema = df_to_schema(df)\n",
        "        result.append({\n",
        "            \"file\": entry,\n",
        "            \"rows_sampled\": len(df),\n",
        "            \"columns\": schema\n",
        "        })\n",
        "\n",
        "    return json.dumps({\"folder\": folder_path, \"files\": result}, indent=2)"
      ],
      "metadata": {
        "id": "JXfYXunn3zXi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output=read_local_folder_and_infer_schema(\"/content/sample_data/files\")"
      ],
      "metadata": {
        "id": "LybLjttL34EL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ---------- Helper: map pandas dtype -> Snowflake type ----------\n",
        "def map_dtype_to_snowflake(dtype: str, sample_values: List[Any]) -> str:\n",
        "    \"\"\"\n",
        "    Simple mapping rules. Expand as needed for production.\n",
        "    \"\"\"\n",
        "    # normalize dtype\n",
        "    d = dtype.lower()\n",
        "    if \"int\" in d:\n",
        "        return \"NUMBER\"\n",
        "    if \"float\" in d or \"double\" in d or \"decimal\" in d:\n",
        "        return \"FLOAT\"\n",
        "    if \"bool\" in d or \"boolean\" in d:\n",
        "        return \"BOOLEAN\"\n",
        "    # date/datetime detection via dtype or sample inspection\n",
        "    if \"datetime\" in d or \"timestamp\" in d:\n",
        "        return \"TIMESTAMP_NTZ\"\n",
        "    if \"date\" in d:\n",
        "        return \"DATE\"\n",
        "\n",
        "    # otherwise treat as text; try to detect length from samples\n",
        "    max_len = 0\n",
        "    for v in sample_values:\n",
        "        try:\n",
        "            max_len = max(max_len, len(str(v)))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if max_len <= 256 and max_len > 0:\n",
        "        return f\"VARCHAR({max_len})\"\n",
        "    return \"TEXT\""
      ],
      "metadata": {
        "id": "LVduF7NuDJpH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"generate_snowflake_ddl\")\n",
        "def generate_snowflake_ddl(schema_json_str: str, table_name_hint: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Input: JSON string produced by `read_local_folder_and_infer_schema`.\n",
        "    Output: a textual DDL for Snowflake CREATE TABLE statements.\n",
        "    \"\"\"\n",
        "    data = json.loads(schema_json_str)\n",
        "    files = data.get(\"files\", [])\n",
        "    ddl_texts = []\n",
        "\n",
        "    for f in files:\n",
        "        if \"error\" in f:\n",
        "            continue\n",
        "        file_name = f.get(\"file\")\n",
        "        if table_name_hint:\n",
        "            tbl = table_name_hint\n",
        "        else:\n",
        "            # derive table name from file name (remove extension, non-alphanum -> _)\n",
        "            base = os.path.splitext(file_name)[0]\n",
        "            tbl = \"\".join([c if c.isalnum() else \"_\" for c in base]).upper()[:128]\n",
        "\n",
        "        columns = f.get(\"columns\", [])\n",
        "        col_lines = []\n",
        "        for col in columns:\n",
        "            col_name = col[\"column\"]\n",
        "            sf_type = map_dtype_to_snowflake(col[\"dtype\"], col.get(\"sample_values\", []))\n",
        "            nullable = \"\" if not col.get(\"nullable\", False) else \"NULL\"\n",
        "            # default to NOT NULL if no nulls, else allow NULL\n",
        "            null_clause = \"NOT NULL\" if not col.get(\"nullable\", False) else \"NULL\"\n",
        "            col_line = f'  \"{col_name.upper()}\" {sf_type} {null_clause}'\n",
        "            col_lines.append(col_line)\n",
        "\n",
        "        ddl = f'CREATE OR REPLACE TABLE {tbl} (\\n' + \",\\n\".join(col_lines) + \"\\n);\\n-- source file: \" + file_name\n",
        "        ddl_texts.append(ddl)\n",
        "\n",
        "    return \"\\n\\n\".join(ddl_texts)"
      ],
      "metadata": {
        "id": "iC7yicbJDUDg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_KEY2')"
      ],
      "metadata": {
        "id": "eXAaMBGkFGBB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "llm = init_chat_model('openai:gpt-4o-mini')"
      ],
      "metadata": {
        "id": "CCrzlfQiES7a"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def build_agents_and_run(folder_path: str, table_name_hint: str = None):\n",
        "#     \"\"\"\n",
        "#     Example orchestration:\n",
        "#      1) Use agent A's tool to get schema\n",
        "#      2) Pass schema JSON into agent B tool to get DDL\n",
        "#     \"\"\"\n",
        "#     # Build a simple agent: specify the model name according to your provider.\n",
        "#     # For example: model=\"openai:gpt-5\" or \"openai:gpt-4o\"\n",
        "#     # You must have the provider API keys configured in env for your model.\n",
        "#     agent = create_agent(\n",
        "#         model=llm,  # replace with your model identifier\n",
        "#         tools=[read_local_folder_and_infer_schema, generate_snowflake_ddl],\n",
        "#         system_prompt=SystemMessage(content=\"You are an assistant that calls the tools to inspect local files and produce Snowflake DDL. Use the tools rather than inventing file contents.\")\n",
        "#     )\n",
        "\n",
        "#     print(\"\\n=== Agent-run (agent may call tools interactively) ===\")\n",
        "\n",
        "#     question = f\"Please inspect files in {folder_path} and generate Snowflake DDL for the files in the folder.\"\n",
        "#     for step in agent.stream(\n",
        "#         {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
        "#         stream_mode=\"values\",\n",
        "#     ):\n",
        "#         step[\"messages\"][-1].pretty_print()\n",
        "# if __name__ == \"__main__\":\n",
        "#     folder = \"/content/sample_data/files\"  # change to your folder\n",
        "#     build_agents_and_run(folder, table_name_hint=None)\n"
      ],
      "metadata": {
        "id": "q5sOrBn24PxY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SF_DDL(BaseModel):\n",
        "  DDL: List[str] = Field(description=\"List of DDL for Snowflake DB\")"
      ],
      "metadata": {
        "id": "xFnFDSk5KKkM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path=\"/content/sample_data/files\"\n",
        "agent1 = create_agent(\n",
        "    model=llm,  # replace with your model identifier\n",
        "    tools=[read_local_folder_and_infer_schema, generate_snowflake_ddl],\n",
        "    system_prompt=SystemMessage(content=\"You are an assistant that calls the tools to inspect local files and produce Snowflake DDL. Use the tools rather than inventing file contents.\"),\n",
        "    response_format=SF_DDL\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "0yy0FtnX4kUO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = f\"Please inspect files in {folder_path} and generate Snowflake DDL for the files in the folder.\"\n",
        "reponse=agent1.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})"
      ],
      "metadata": {
        "id": "KdB33mKY4kMO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_snowflake_tables(ddl: str) -> None:\n",
        "  with open(\"snowflake_config.yaml\", \"r\") as f:\n",
        "      config = yaml.safe_load(f)\n",
        "\n",
        "  sf = config[\"snowflake\"]\n",
        "\n",
        "  # Build Snowflake connection URI\n",
        "  connection_uri = (\n",
        "      f\"snowflake://{sf['user']}:{sf['password']}@{sf['account']}/\"\n",
        "      f\"{sf['database']}/{sf['schema']}?warehouse={sf['warehouse']}\"\n",
        "  )\n",
        "\n",
        "  # If role is included\n",
        "  if \"role\" in sf:\n",
        "      connection_uri += f\"&role={sf['role']}\"\n",
        "\n",
        "  #print(\"Snowflake URI:\", connection_uri)\n",
        "\n",
        "  db = SQLDatabase.from_uri(connection_uri)\n",
        "  db.run(ddl)\n",
        "  print(\"Done!\" , f\"create sf table for {ddl}\")"
      ],
      "metadata": {
        "id": "S3jCa7EsTnya"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ddl in reponse['structured_response'].DDL:\n",
        "  print(create_snowflake_tables(ddl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLR34CN3MFD1",
        "outputId": "9dc5f07e-9322-4314-93db-9ef24b6054be"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATE OR REPLACE TABLE DIM_Customer (CustomerKey FLOAT, \"First Name\" STRING NOT NULL, \"Last Name\" STRING NOT NULL, \"Full Name\" STRING NOT NULL, Gender STRING NOT NULL, DateFirstPurchase STRING NOT NULL, \"Customer City\" STRING);\n",
            "CREATE OR REPLACE TABLE DIM_Product (ProductKey FLOAT, ProductItemCode STRING NOT NULL, \"Product Name\" STRING NOT NULL, \"Sub Category\" STRING, \"Product Category\" STRING, \"Product Color\" STRING, \"Product Size\" STRING, \"Product Line\" STRING, \"Product Model Name\" STRING, \"Product Description\" STRING, \"Product Status\" STRING NOT NULL);\n",
            "CREATE OR REPLACE TABLE FACT_InternetSales (ProductKey INT NOT NULL, OrderDateKey INT NOT NULL, DueDateKey INT NOT NULL, ShipDateKey INT NOT NULL, CustomerKey INT NOT NULL, SalesOrderNumber STRING NOT NULL, SalesAmount FLOAT NOT NULL);\n"
          ]
        }
      ]
    }
  ]
}